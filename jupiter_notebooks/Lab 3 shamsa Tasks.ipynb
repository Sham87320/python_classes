{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbbbe27",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a6078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import string \n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24819e",
   "metadata": {},
   "source": [
    "# Extract Data using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb6a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/\"\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "  soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "    \n",
    "text_list = [] #### Explain mine############\n",
    "\n",
    "# Find all elements with the class 'paragraph'\n",
    "paragraphs = soup.find_all('p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1602e74",
   "metadata": {},
   "source": [
    "# Extract text from paragraph and store in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dde6e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are mainly two ways to extract data from a website:', 'This article discusses the steps involved in web scraping using the implementation of a Web Scraping framework of Python called Beautiful Soup. Steps involved in web scraping:', 'Step 1: Installing the required third-party libraries', 'Step 2: Accessing the HTML content from webpage\\xa0', 'Let us try to understand this piece of code.', 'Note: Sometimes you may get error “Not accepted” so try adding a browser user agent like below. Find your user agent based on device and browser from here https://deviceatlas.com/blog/list-of-user-agent-strings', 'Step 3: Parsing the HTML content\\xa0', 'A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So \\xa0BeautifulSoup object and specify the parser library can be created at the same time. In the example above,', 'We create a BeautifulSoup object by passing two arguments:', 'Now soup.prettify() is printed,\\xa0it gives the visual representation of the parse tree created from the raw HTML content. Step 4: Searching and navigating through the parse tree Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them).\\xa0', 'Before moving on, we recommend you to go through the HTML content of the webpage which we printed using soup.prettify() method and try to find a pattern or a way to navigate to the quotes.', 'So, this was a simple example of how to create a web scraper in Python. \\xa0From here, you can try to scrap any other website of your choice. In case of any queries, post them below in comments section.', 'Note : Web Scraping is considered as illegal in many cases. It may also cause your IP to be blocked permanently by a website. This blog is contributed by Nikhil Kumar.', 'BeautifulSoup is a powerful library in Python used for web scraping purposes. It helps in parsing HTML and XML documents, making it easy to navigate, search, and modify the parse tree.', 'This script fetches an HTML page, parses it, and prints out all text within <h1> tags.', 'BeautifulSoup is used for parsing HTML and XML documents. It is commonly used to extract information from web pages, such as gathering data from various elements within the HTML structure of the page.', 'Yes, BeautifulSoup is excellent for web scraping due to its ability to parse complex HTML content and its user-friendly methods for navigating and searching the parse tree. It can handle various markup formats and provides simple methods for extracting data, which makes it a popular choice among Python developers for web scraping tasks.', 'The name “BeautifulSoup” is whimsical, reflecting the library’s ability to turn “markup soup” — which is often how real-world HTML documents are described due to their messy and inconsistent nature — into a structured format that is easy to work with. It effectively “beautifies” this “soup” of code.', 'The best tool for web scraping often depends on the specific needs of the project:', 'Each tool has its strengths and is best suited to different types of scraping tasks. BeautifulSoup is often enough for basic projects and is an excellent tool for beginners due to its simplicity and ease of use.', '']\n"
     ]
    }
   ],
   "source": [
    "# Extract text from each paragraph and store in a list\n",
    "text_list = [paragraph.text for paragraph in paragraphs]\n",
    "\n",
    "# Print the list of extracted text\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bac92d1-8aa5-4f1a-bab6-9ba85fdc8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are mainly two ways to extract data from a website:', 'This article discusses the steps involved in web scraping using the implementation of a Web Scraping framework of Python called Beautiful Soup. Steps involved in web scraping:', 'Step 1: Installing the required third-party libraries', 'Step 2: Accessing the HTML content from webpage\\xa0', 'Let us try to understand this piece of code.', 'Note: Sometimes you may get error “Not accepted” so try adding a browser user agent like below. Find your user agent based on device and browser from here https://deviceatlas.com/blog/list-of-user-agent-strings', 'Step 3: Parsing the HTML content\\xa0', 'A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So \\xa0BeautifulSoup object and specify the parser library can be created at the same time. In the example above,', 'We create a BeautifulSoup object by passing two arguments:', 'Now soup.prettify() is printed,\\xa0it gives the visual representation of the parse tree created from the raw HTML content. Step 4: Searching and navigating through the parse tree Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them).\\xa0', 'Before moving on, we recommend you to go through the HTML content of the webpage which we printed using soup.prettify() method and try to find a pattern or a way to navigate to the quotes.', 'So, this was a simple example of how to create a web scraper in Python. \\xa0From here, you can try to scrap any other website of your choice. In case of any queries, post them below in comments section.', 'Note : Web Scraping is considered as illegal in many cases. It may also cause your IP to be blocked permanently by a website. This blog is contributed by Nikhil Kumar.', 'BeautifulSoup is a powerful library in Python used for web scraping purposes. It helps in parsing HTML and XML documents, making it easy to navigate, search, and modify the parse tree.', 'This script fetches an HTML page, parses it, and prints out all text within <h1> tags.', 'BeautifulSoup is used for parsing HTML and XML documents. It is commonly used to extract information from web pages, such as gathering data from various elements within the HTML structure of the page.', 'Yes, BeautifulSoup is excellent for web scraping due to its ability to parse complex HTML content and its user-friendly methods for navigating and searching the parse tree. It can handle various markup formats and provides simple methods for extracting data, which makes it a popular choice among Python developers for web scraping tasks.', 'The name “BeautifulSoup” is whimsical, reflecting the library’s ability to turn “markup soup” — which is often how real-world HTML documents are described due to their messy and inconsistent nature — into a structured format that is easy to work with. It effectively “beautifies” this “soup” of code.', 'The best tool for web scraping often depends on the specific needs of the project:', 'Each tool has its strengths and is best suited to different types of scraping tasks. BeautifulSoup is often enough for basic projects and is an excellent tool for beginners due to its simplicity and ease of use.', '']\n"
     ]
    }
   ],
   "source": [
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b9d25",
   "metadata": {},
   "source": [
    "# Remove punctuation from each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfd822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove_punctuation = lambda x: re.sub(r'[^\\w\\s]', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b09519-ad86-4226-9f9a-721e3e64f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are mainly two ways to extract data from a website', 'This article discusses the steps involved in web scraping using the implementation of a Web Scraping framework of Python called Beautiful Soup Steps involved in web scraping', 'Step 1 Installing the required thirdparty libraries', 'Step 2 Accessing the HTML content from webpage\\xa0', 'Let us try to understand this piece of code', 'Note Sometimes you may get error “Not accepted” so try adding a browser user agent like below Find your user agent based on device and browser from here httpsdeviceatlascombloglistofuseragentstrings', 'Step 3 Parsing the HTML content\\xa0', 'A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib lxml htmlparser etc So \\xa0BeautifulSoup object and specify the parser library can be created at the same time In the example above', 'We create a BeautifulSoup object by passing two arguments', 'Now soupprettify is printed\\xa0it gives the visual representation of the parse tree created from the raw HTML content Step 4 Searching and navigating through the parse tree Now we would like to extract some useful data from the HTML content The soup object contains all the data in the nested structure which could be programmatically extracted In our example we are scraping a webpage consisting of some quotes So we would like to create a program to save those quotes and all relevant information about them\\xa0', 'Before moving on we recommend you to go through the HTML content of the webpage which we printed using soupprettify method and try to find a pattern or a way to navigate to the quotes', 'So this was a simple example of how to create a web scraper in Python \\xa0From here you can try to scrap any other website of your choice In case of any queries post them below in comments section', 'Note  Web Scraping is considered as illegal in many cases It may also cause your IP to be blocked permanently by a website This blog is contributed by Nikhil Kumar', 'BeautifulSoup is a powerful library in Python used for web scraping purposes It helps in parsing HTML and XML documents making it easy to navigate search and modify the parse tree', 'This script fetches an HTML page parses it and prints out all text within h1 tags', 'BeautifulSoup is used for parsing HTML and XML documents It is commonly used to extract information from web pages such as gathering data from various elements within the HTML structure of the page', 'Yes BeautifulSoup is excellent for web scraping due to its ability to parse complex HTML content and its userfriendly methods for navigating and searching the parse tree It can handle various markup formats and provides simple methods for extracting data which makes it a popular choice among Python developers for web scraping tasks', 'The name “BeautifulSoup” is whimsical reflecting the library’s ability to turn “markup soup” — which is often how realworld HTML documents are described due to their messy and inconsistent nature — into a structured format that is easy to work with It effectively “beautifies” this “soup” of code', 'The best tool for web scraping often depends on the specific needs of the project', 'Each tool has its strengths and is best suited to different types of scraping tasks BeautifulSoup is often enough for basic projects and is an excellent tool for beginners due to its simplicity and ease of use', '']\n"
     ]
    }
   ],
   "source": [
    "text_list = [''.join(c for c in s if c not in string.punctuation) for s in text_list]\n",
    "print(text_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a243c3",
   "metadata": {},
   "source": [
    "# Convert clean data into datafram and use dataframe column name 'text without punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3affe56-b481-4d17-a985-0b94dc270200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text without punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are mainly two ways to extract data from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This article discusses the steps involved in w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step 1 Installing the required thirdparty libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step 2 Accessing the HTML content from webpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let us try to understand this piece of code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text without punctuation\n",
       "0  There are mainly two ways to extract data from...\n",
       "1  This article discusses the steps involved in w...\n",
       "2  Step 1 Installing the required thirdparty libr...\n",
       "3    Step 2 Accessing the HTML content from webpage \n",
       "4        Let us try to understand this piece of code"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling DataFrame constructor on list\n",
    "df = pd.DataFrame(text_list)\n",
    "df.columns =['text without punctuation']\n",
    "df.head() ### meaning please####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b4e05",
   "metadata": {},
   "source": [
    "# Save datafram into csv file 'text without punctuation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22d63e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('Lab 3 shamsa Taks.csv')\n",
    "df.to_csv('Lab 3 shamsa Taks.csv', index=False)#(explain index=Flase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684ebb3",
   "metadata": {},
   "source": [
    "#  Read file 'text without punctuation.csv' again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16aa5179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text without punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There are mainly two ways to extract data from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This article discusses the steps involved in w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step 1 Installing the required thirdparty libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step 2 Accessing the HTML content from webpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let us try to understand this piece of code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Note Sometimes you may get error “Not accepted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step 3 Parsing the HTML content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A really nice thing about the BeautifulSoup li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We create a BeautifulSoup object by passing tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Now soupprettify is printed it gives the visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Before moving on we recommend you to go throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>So this was a simple example of how to create ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Note  Web Scraping is considered as illegal in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BeautifulSoup is a powerful library in Python ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This script fetches an HTML page parses it and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BeautifulSoup is used for parsing HTML and XML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yes BeautifulSoup is excellent for web scrapin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The name “BeautifulSoup” is whimsical reflecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The best tool for web scraping often depends o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Each tool has its strengths and is best suited...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text without punctuation\n",
       "0   There are mainly two ways to extract data from...\n",
       "1   This article discusses the steps involved in w...\n",
       "2   Step 1 Installing the required thirdparty libr...\n",
       "3     Step 2 Accessing the HTML content from webpage \n",
       "4         Let us try to understand this piece of code\n",
       "5   Note Sometimes you may get error “Not accepted...\n",
       "6                    Step 3 Parsing the HTML content \n",
       "7   A really nice thing about the BeautifulSoup li...\n",
       "8   We create a BeautifulSoup object by passing tw...\n",
       "9   Now soupprettify is printed it gives the visua...\n",
       "10  Before moving on we recommend you to go throug...\n",
       "11  So this was a simple example of how to create ...\n",
       "12  Note  Web Scraping is considered as illegal in...\n",
       "13  BeautifulSoup is a powerful library in Python ...\n",
       "14  This script fetches an HTML page parses it and...\n",
       "15  BeautifulSoup is used for parsing HTML and XML...\n",
       "16  Yes BeautifulSoup is excellent for web scrapin...\n",
       "17  The name “BeautifulSoup” is whimsical reflecti...\n",
       "18  The best tool for web scraping often depends o...\n",
       "19  Each tool has its strengths and is best suited...\n",
       "20                                                   "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9f417",
   "metadata": {},
   "source": [
    "# Remove stopword using lambda function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96e011fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"the\", \"and\", \"is\", \"in\", \"it\", \"to\", \"with\", \"this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af3cea7-b328-41e2-a05e-4253fc223954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Package 'the' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download([\"the\", \"and\", \"is\", \"in\", \"it\", \"to\", \"with\", \"this\"])\n",
    "Translated_text = stopwords.words(\"english\")\n",
    "print(Translated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eea921",
   "metadata": {},
   "source": [
    "# Add text without stopwords into existing dataframe with column name 'text without punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07e9828b-3b96-4898-a489-2966230a815c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(text_list)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add a new column \"text without punctuation\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext without punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (1) does not match length of index (21)"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#df = pd.DataFrame(text_list)\n",
    "\n",
    "# Add a new column \"text without punctuation\"\n",
    "#df[\"text without punctuation\"] = [\"Translated_text\"]\n",
    "\n",
    "#print(df)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a sample sentence with stop words\"\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_text = [word for word in nltk.word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "# Create a DataFrame with the original text and filtered text\n",
    "data = {'original_text': [text], 'text without punctuation': [filtered_text]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a267eea",
   "metadata": {},
   "source": [
    "# Save datafram into json file 'text without stopwords.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "316942fb",
   "metadata": {},
   "source": [
    "# Read file 'text without stopwords.json' again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043996d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ecb6c97",
   "metadata": {},
   "source": [
    "# Remove numbers using regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9c2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f40e6b2f",
   "metadata": {},
   "source": [
    "# Save datafram into xml file 'text without numbers.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4df2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf2ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
