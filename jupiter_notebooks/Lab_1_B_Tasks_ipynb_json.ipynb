{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0iwtDjOweq1E"
   },
   "outputs": [],
   "source": [
    "# Reading data from a file\n",
    "with open('C:\\\\Users\\\\shams\\\\OneDrive\\\\Dokument\\\\Part time\\\\Sample Text.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "#\"C:\\Users\\shams\\Downloads\\sample1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "xww-LBt5gNjG",
    "outputId": "3782d45a-c381-4017-ac0c-6ce55e93c1a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is an amazing day or give it a try. Do not stop at it \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBQM6tZKeq1G"
   },
   "source": [
    "# Read a File and Count Words\n",
    "Task: Write a program that reads a text file which you created in lab 1 A and counts the total number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSoXZFVTeq1O",
    "outputId": "0eac952e-8154-4f62-d005-44208d0f77e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(data.split())\n",
    "count_words = data.split()\n",
    "len(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtlBFQmUeq1P"
   },
   "source": [
    "# Find the Longest Word in a File\n",
    "Task: Find the longest word in the file and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "uYD1o1UGeq1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing\n"
     ]
    }
   ],
   "source": [
    "def longest_word(data):\n",
    "          words = infile.read().split()\n",
    "#print(words)    #  return list ['What', 'is', 'Python', 'language?', 'Python', ……..]\n",
    "max_len = len(max(words, key=len))\n",
    "print(longest)\n",
    "\n",
    "\n",
    "#def longest_word(data):\n",
    "#with open(data, 'r') as infile:\n",
    "          #words = infile.read().split()\n",
    "#print(words)    #  return list ['What', 'is', 'Python', 'language?', 'Python', ……..]\n",
    "#max_len = len(max(words, key=len))\n",
    "  #return[word for word in words if len(word) == max_len]\n",
    "#print(longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zqc3th-0eq1S",
    "outputId": "3a96f31c-1a2c-442c-e087-a4b2bc880b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing\n"
     ]
    }
   ],
   "source": [
    "words = data.split()\n",
    "longest = max(words, key=len)\n",
    "print(longest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOSi1RU8eq1T"
   },
   "source": [
    "# Count the Occurrence of Each Word\n",
    "Task: Count how many times each word occurs in the file and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'This': 1, 'is': 1, 'an': 1, 'amazing': 1, 'day': 1, 'or': 1, 'give': 1, 'it': 2, 'a': 1, 'try.': 1, 'Do': 1, 'not': 1, 'stop': 1, 'at': 1, '\\n': 1}\n"
     ]
    }
   ],
   "source": [
    "words = data.split(' ')\n",
    "from collections import Counter    \n",
    "result = dict(Counter(words))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fewrcmEjeq1X"
   },
   "source": [
    "# Delete Blank Lines from a File\n",
    "Task: Remove any blank lines from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "HIr2vxcieq1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an amazing day or give it a try. Do not stop at it \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Opening the file before deleting the blank lines \n",
    "with open('data', 'w') as f: \n",
    "    f.write(data) \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "CGjl6r-Keq1X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_2:\n",
      " This is an amazing day or give it a try. Do not stop at it \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# opening and creating new .txt file \n",
    "with open( \n",
    "\t'data', 'r') as r, open( \n",
    "\t\t'output.txt', 'w') as o: \n",
    "\t\n",
    "\tfor line in r: \n",
    "\t\t#strip() function \n",
    "\t\tif line.strip(): \n",
    "\t\t\to.write(line) \n",
    "\n",
    "f = open('output.txt', 'r') \n",
    "print('data_2:\\n',f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Cns0ncIeq1X"
   },
   "source": [
    "# Word Frequency Analysis\n",
    "Task: Perform word frequency analysis and display the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "dyO2qeOyeq1Y"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence:  This is an amazing day or give it a try. Do not stop at it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do: 1\n",
      "This: 1\n",
      "a: 1\n",
      "amazing: 1\n",
      "an: 1\n",
      "at: 1\n",
      "day: 1\n",
      "give: 1\n",
      "is: 1\n",
      "it: 2\n",
      "not: 1\n",
      "or: 1\n",
      "stop: 1\n",
      "try.: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequencies(words):\n",
    " \n",
    "    return  Counter(words.split(\" \"))\n",
    "\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    words = input(\"Enter a sentence: \")\n",
    "    your_dictionary = word_frequencies(words)\n",
    "    sorted_keys = sorted(your_dictionary.keys())\n",
    "    for key in sorted_keys:\n",
    "        print(key + ': ' + str(your_dictionary[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX363l6Deq1Y"
   },
   "source": [
    "# Remove Stopwords\n",
    "the, or, an, is, a, it, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'amazing', 'day', 'give', 'try', '.', 'Do', 'stop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(data)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word not in stop_words] #filter out stopwords from the list\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlZZfp5Teq1Z"
   },
   "source": [
    "# Find the Top 5 Longest Sentences in a File\n",
    "Task: Find the five longest sentences in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an amazing day or give it a try.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#text = \"this is a sentence. this is also a sentence. this is too.\"\n",
    "sentences = nltk.sent_tokenize(data)\n",
    "print(max(sentences, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is an amazing day or give it a try.', 'Do not stop at it']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def find_longest_sentences(data):\n",
    "  \"\"\"Finds the top 5 longest sentences in a text file.\n",
    "\n",
    "  Args:\n",
    "    filename: The name of the text file.\n",
    "\n",
    "  Returns:\n",
    "    A list of the top 5 longest sentences.\n",
    "  \"\"\"\n",
    "\n",
    "  sentences = sent_tokenize(data)\n",
    "\n",
    "  longest_sentences = []\n",
    "  for sentence in sentences:\n",
    "    if len(longest_sentences) < 5:\n",
    "      longest_sentences.append(sentence)\n",
    "    else:\n",
    "      min_length = min(len(s) for s in longest_sentences)\n",
    "      if len(sentence) > min_length:\n",
    "        longest_sentences.remove(min(longest_sentences, key=len))\n",
    "        longest_sentences.append(sentence)\n",
    "\n",
    "  return longest_sentences\n",
    "print(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
